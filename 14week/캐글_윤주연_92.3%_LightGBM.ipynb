{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337af804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6671fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Electronics_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56170c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>BP</th>\n",
       "      <th>blue</th>\n",
       "      <th>c_speed</th>\n",
       "      <th>dual</th>\n",
       "      <th>front_c</th>\n",
       "      <th>4G</th>\n",
       "      <th>m_int</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>m_wt</th>\n",
       "      <th>...</th>\n",
       "      <th>px_h</th>\n",
       "      <th>px_w</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_t</th>\n",
       "      <th>3G</th>\n",
       "      <th>ts</th>\n",
       "      <th>wifi</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>188</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>756</td>\n",
       "      <td>2549</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>905</td>\n",
       "      <td>1988</td>\n",
       "      <td>2631</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>563</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9</td>\n",
       "      <td>145</td>\n",
       "      <td>...</td>\n",
       "      <td>1263</td>\n",
       "      <td>1716</td>\n",
       "      <td>2603</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>615</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>131</td>\n",
       "      <td>...</td>\n",
       "      <td>1216</td>\n",
       "      <td>1786</td>\n",
       "      <td>2769</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1821</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>141</td>\n",
       "      <td>...</td>\n",
       "      <td>1208</td>\n",
       "      <td>1212</td>\n",
       "      <td>1411</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    BP  blue  c_speed  dual  front_c  4G  m_int  m_dep  m_wt  \\\n",
       "0           0   842     0      2.2     0        1   0      7    0.6   188   \n",
       "1           1  1021     1      0.5     1        0   1     53    0.7   136   \n",
       "2           2   563     1      0.5     1        2   1     41    0.9   145   \n",
       "3           3   615     1      2.5     0        0   0     10    0.8   131   \n",
       "4           4  1821     1      1.2     0       13   1     44    0.6   141   \n",
       "\n",
       "   ...  px_h  px_w   ram  sc_h  sc_w  talk_t  3G  ts  wifi  target  \n",
       "0  ...    20   756  2549     9     7      19   0   0     1       1  \n",
       "1  ...   905  1988  2631    17     3       7   1   1     0       2  \n",
       "2  ...  1263  1716  2603    11     2       9   1   1     0       2  \n",
       "3  ...  1216  1786  2769    16     8      11   1   0     0       2  \n",
       "4  ...  1208  1212  1411     8     2      15   1   1     0       1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "294b02b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc276e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    379\n",
       "0    375\n",
       "2    374\n",
       "1    372\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "466cdcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 21 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   BP       1500 non-null   int64  \n",
      " 1   blue     1500 non-null   int64  \n",
      " 2   c_speed  1500 non-null   float64\n",
      " 3   dual     1500 non-null   int64  \n",
      " 4   front_c  1500 non-null   int64  \n",
      " 5   4G       1500 non-null   int64  \n",
      " 6   m_int    1500 non-null   int64  \n",
      " 7   m_dep    1500 non-null   float64\n",
      " 8   m_wt     1500 non-null   int64  \n",
      " 9   n_cores  1500 non-null   int64  \n",
      " 10  prim_c   1500 non-null   int64  \n",
      " 11  px_h     1500 non-null   int64  \n",
      " 12  px_w     1500 non-null   int64  \n",
      " 13  ram      1500 non-null   int64  \n",
      " 14  sc_h     1500 non-null   int64  \n",
      " 15  sc_w     1500 non-null   int64  \n",
      " 16  talk_t   1500 non-null   int64  \n",
      " 17  3G       1500 non-null   int64  \n",
      " 18  ts       1500 non-null   int64  \n",
      " 19  wifi     1500 non-null   int64  \n",
      " 20  target   1500 non-null   int64  \n",
      "dtypes: float64(2), int64(19)\n",
      "memory usage: 246.2 KB\n"
     ]
    }
   ],
   "source": [
    "data.info() #결측값 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22de5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = data.iloc[:,:-1]\n",
    "y_data = data.iloc[:,-1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22861a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "sorted_i = rf.feature_importances_.argsort()\n",
    "sns.barplot(x=rf.feature_importances_[sorted_i], y=data.columns[sorted_i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "482e77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 하위 6개 제거\n",
    "\n",
    "data.drop(['3G', 'blue', 'dual', 'wifi', '4G', 'ts'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66afe75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardsacler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard = StandardScaler()\n",
    "X_scaled_standard = standard.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01224252",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_standard, y_data, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b525f58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier():  0.8233\n",
      "GaussianNB():  0.8167\n",
      "RandomForestClassifier():  0.8533\n",
      "AdaBoostClassifier():  0.7233\n",
      "GradientBoostingClassifier():  0.8967\n",
      "[15:53:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None):  0.9133\n",
      "LGBMClassifier():  0.9000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "GNB = GaussianNB()\n",
    "rf = RandomForestClassifier()\n",
    "ada = AdaBoostClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "xgb = XGBClassifier()\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "for model in [tree, GNB, rf, ada, gb, xgb, lgbm]:\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_model = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred_model)\n",
    "    \n",
    "    print('{0}: {1: .4f}'.format(model, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16852385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#이상치 제거\n",
    "\n",
    "#-1. IQR로 아웃라이어 제거 (성윤 캐글 참고)\n",
    "\n",
    "def hello_outlier(df=None, column=None, weight=1.5) :\n",
    "    quantile_25 = np.percentile(df[column].values, 25)\n",
    "    quantile_75 = np.percentile(df[column].values, 75)\n",
    "\n",
    "    IQR = quantile_75 - quantile_25\n",
    "    IQR_weight = IQR*weight\n",
    "\n",
    "    lowest = quantile_25 - IQR_weight\n",
    "    highest = quantile_75 + IQR_weight\n",
    "\n",
    "    outlier_idx = df[column][ (df[column] < lowest) | (df[column] > highest) ].index\n",
    "    return outlier_idx\n",
    "\n",
    "def bye_outlier(df=None, column=None, weight=1.5) :\n",
    "    outlier_idx = hello_outlier(df=df, column=column, weight=1.5)\n",
    "    df.drop(outlier_idx, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e410be6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "칼럼이름 :  BP\n",
      "Int64Index([], dtype='int64')\n",
      "칼럼이름 :  c_speed\n",
      "Int64Index([], dtype='int64')\n",
      "칼럼이름 :  front_c\n",
      "Int64Index([95, 169, 226, 229, 300, 305, 372, 584, 1387, 1406, 1416], dtype='int64')\n",
      "칼럼이름 :  m_int\n",
      "Int64Index([], dtype='int64')\n",
      "칼럼이름 :  m_dep\n",
      "Int64Index([], dtype='int64')\n",
      "칼럼이름 :  m_wt\n",
      "Int64Index([], dtype='int64')\n",
      "칼럼이름 :  n_cores\n",
      "Int64Index([], dtype='int64')\n",
      "칼럼이름 :  prim_c\n",
      "Int64Index([], dtype='int64')\n",
      "칼럼이름 :  px_h\n",
      "Int64Index([], dtype='int64')\n",
      "칼럼이름 :  px_w\n",
      "Int64Index([], dtype='int64')\n",
      "칼럼이름 :  ram\n",
      "Int64Index([], dtype='int64')\n",
      "칼럼이름 :  sc_h\n",
      "Int64Index([], dtype='int64')\n",
      "칼럼이름 :  sc_w\n",
      "Int64Index([], dtype='int64')\n",
      "칼럼이름 :  talk_t\n",
      "Int64Index([], dtype='int64')\n",
      "칼럼이름 :  target\n",
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "for i in data.columns :\n",
    "    print('칼럼이름 : ',i)\n",
    "    print(hello_outlier(df=data, column=i, weight=1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "852ac219",
   "metadata": {},
   "outputs": [],
   "source": [
    "bye_outlier(df=data, column='front_c',weight=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e97efd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = data.iloc[:,:-1]\n",
    "y_data = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fee62b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardscaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard = StandardScaler()\n",
    "X_scaled_standard = standard.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59bf2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_standard, y_data, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e677b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier():  0.8389\n",
      "GaussianNB():  0.7919\n",
      "RandomForestClassifier():  0.8758\n",
      "AdaBoostClassifier():  0.7383\n",
      "GradientBoostingClassifier():  0.8960\n",
      "[15:53:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None):  0.9027\n",
      "LGBMClassifier():  0.8893\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "GNB = GaussianNB()\n",
    "rf = RandomForestClassifier()\n",
    "ada = AdaBoostClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "xgb = XGBClassifier()\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "for model in [tree, GNB, rf, ada, gb, xgb, lgbm]:\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_model = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred_model)\n",
    "    \n",
    "    print('{0}: {1: .4f}'.format(model, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2e7335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Electronics_train.csv')\n",
    "data = data = data.iloc[:,1:]\n",
    "data.drop(['3G', 'blue', 'dual', 'wifi', '4G', 'ts'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e86ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-2 z score(표준화 후 이상치 제거)\n",
    "\n",
    "X_data = data.iloc[:,:-1]\n",
    "y_data = data.iloc[:,-1]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b191b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f2e3c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BP</th>\n",
       "      <th>c_speed</th>\n",
       "      <th>front_c</th>\n",
       "      <th>m_int</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>m_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>prim_c</th>\n",
       "      <th>px_h</th>\n",
       "      <th>px_w</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.908428</td>\n",
       "      <td>0.832539</td>\n",
       "      <td>-0.759235</td>\n",
       "      <td>-1.408691</td>\n",
       "      <td>0.350508</td>\n",
       "      <td>1.336082</td>\n",
       "      <td>-1.090027</td>\n",
       "      <td>-1.296378</td>\n",
       "      <td>-1.418046</td>\n",
       "      <td>-1.150394</td>\n",
       "      <td>0.384162</td>\n",
       "      <td>-0.793792</td>\n",
       "      <td>0.287837</td>\n",
       "      <td>1.460049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.500831</td>\n",
       "      <td>-1.252078</td>\n",
       "      <td>-0.989167</td>\n",
       "      <td>1.125718</td>\n",
       "      <td>0.700550</td>\n",
       "      <td>-0.141340</td>\n",
       "      <td>-0.654365</td>\n",
       "      <td>-0.637705</td>\n",
       "      <td>0.576155</td>\n",
       "      <td>1.724027</td>\n",
       "      <td>0.458924</td>\n",
       "      <td>1.110167</td>\n",
       "      <td>-0.636199</td>\n",
       "      <td>-0.763752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.543732</td>\n",
       "      <td>-1.252078</td>\n",
       "      <td>-0.529303</td>\n",
       "      <td>0.464568</td>\n",
       "      <td>1.400633</td>\n",
       "      <td>0.114368</td>\n",
       "      <td>0.216960</td>\n",
       "      <td>-0.637705</td>\n",
       "      <td>1.382849</td>\n",
       "      <td>1.089415</td>\n",
       "      <td>0.433395</td>\n",
       "      <td>-0.317802</td>\n",
       "      <td>-0.867208</td>\n",
       "      <td>-0.393119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.425324</td>\n",
       "      <td>1.200412</td>\n",
       "      <td>-0.989167</td>\n",
       "      <td>-1.243403</td>\n",
       "      <td>1.050591</td>\n",
       "      <td>-0.283400</td>\n",
       "      <td>0.652622</td>\n",
       "      <td>-0.143700</td>\n",
       "      <td>1.276942</td>\n",
       "      <td>1.252734</td>\n",
       "      <td>0.584743</td>\n",
       "      <td>0.872172</td>\n",
       "      <td>0.518846</td>\n",
       "      <td>-0.022485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.320830</td>\n",
       "      <td>-0.393706</td>\n",
       "      <td>1.999947</td>\n",
       "      <td>0.629856</td>\n",
       "      <td>0.350508</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>-1.090027</td>\n",
       "      <td>0.679641</td>\n",
       "      <td>1.258915</td>\n",
       "      <td>-0.086485</td>\n",
       "      <td>-0.653392</td>\n",
       "      <td>-1.031787</td>\n",
       "      <td>-0.867208</td>\n",
       "      <td>0.718782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         BP   c_speed   front_c     m_int     m_dep      m_wt   n_cores  \\\n",
       "0 -0.908428  0.832539 -0.759235 -1.408691  0.350508  1.336082 -1.090027   \n",
       "1 -0.500831 -1.252078 -0.989167  1.125718  0.700550 -0.141340 -0.654365   \n",
       "2 -1.543732 -1.252078 -0.529303  0.464568  1.400633  0.114368  0.216960   \n",
       "3 -1.425324  1.200412 -0.989167 -1.243403  1.050591 -0.283400  0.652622   \n",
       "4  1.320830 -0.393706  1.999947  0.629856  0.350508  0.000720 -1.090027   \n",
       "\n",
       "     prim_c      px_h      px_w       ram      sc_h      sc_w    talk_t  \n",
       "0 -1.296378 -1.418046 -1.150394  0.384162 -0.793792  0.287837  1.460049  \n",
       "1 -0.637705  0.576155  1.724027  0.458924  1.110167 -0.636199 -0.763752  \n",
       "2 -0.637705  1.382849  1.089415  0.433395 -0.317802 -0.867208 -0.393119  \n",
       "3 -0.143700  1.276942  1.252734  0.584743  0.872172  0.518846 -0.022485  \n",
       "4  0.679641  1.258915 -0.086485 -0.653392 -1.031787 -0.867208  0.718782  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ed8ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = pd.concat([X_scaled_df, y_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44c6843e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BP\n",
      "Int64Index([], dtype='int64') \n",
      "\n",
      "c_speed\n",
      "Int64Index([], dtype='int64') \n",
      "\n",
      "front_c\n",
      "Int64Index([95, 226, 305, 1387, 1406, 1416], dtype='int64') \n",
      "\n",
      "m_int\n",
      "Int64Index([], dtype='int64') \n",
      "\n",
      "m_dep\n",
      "Int64Index([], dtype='int64') \n",
      "\n",
      "m_wt\n",
      "Int64Index([], dtype='int64') \n",
      "\n",
      "n_cores\n",
      "Int64Index([], dtype='int64') \n",
      "\n",
      "prim_c\n",
      "Int64Index([], dtype='int64') \n",
      "\n",
      "px_h\n",
      "Int64Index([], dtype='int64') \n",
      "\n",
      "px_w\n",
      "Int64Index([], dtype='int64') \n",
      "\n",
      "ram\n",
      "Int64Index([], dtype='int64') \n",
      "\n",
      "sc_h\n",
      "Int64Index([], dtype='int64') \n",
      "\n",
      "sc_w\n",
      "Int64Index([], dtype='int64') \n",
      "\n",
      "talk_t\n",
      "Int64Index([], dtype='int64') \n",
      "\n",
      "target\n",
      "Int64Index([], dtype='int64') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#이상치 확인 \n",
    " \n",
    "from scipy import stats\n",
    "\n",
    "for i in scaled_data.columns:\n",
    "    z_score = stats.zscore(scaled_data[i])\n",
    "    print(i)\n",
    "    print(scaled_data[~z_score.between(-3,3)].index,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96f2e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#이상치 제거\n",
    "\n",
    "z_score = stats.zscore(scaled_data['front_c'])\n",
    "standard_data = scaled_data[z_score.between(-3,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "210fda78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = standard_data.iloc[:,:-1]\n",
    "y_data = standard_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a143c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "723dde0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier():  0.8161\n",
      "GaussianNB():  0.8194\n",
      "RandomForestClassifier():  0.8829\n",
      "AdaBoostClassifier():  0.7324\n",
      "GradientBoostingClassifier():  0.9130\n",
      "[15:53:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None):  0.9164\n",
      "LGBMClassifier():  0.9231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "GNB = GaussianNB()\n",
    "rf = RandomForestClassifier()\n",
    "ada = AdaBoostClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "xgb = XGBClassifier()\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "for model in [tree, GNB, rf, ada, gb, xgb, lgbm]:\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_model = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred_model)\n",
    "    \n",
    "    print('{0}: {1: .4f}'.format(model, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fdee8d",
   "metadata": {},
   "source": [
    "Light GBM 최대 : 92.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f10630e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGBM, XGBM, RF feture importance 중 상위 10개 중복 feature 선택(성윤 캐글 참고)\n",
    "\n",
    "sel_data = data[['BP', 'c_speed','m_int', 'm_wt', 'px_h', 'px_w', 'ram', 'talk_t', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28cce69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = sel_data.iloc[:,:-1]\n",
    "y_data = sel_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f0a751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardscaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard = StandardScaler()\n",
    "X_scaled_standard = standard.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f2458ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_standard, y_data, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcf50acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier():  0.8200\n",
      "GaussianNB():  0.7933\n",
      "RandomForestClassifier():  0.8967\n",
      "AdaBoostClassifier():  0.7233\n",
      "GradientBoostingClassifier():  0.9000\n",
      "[15:54:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None):  0.9033\n",
      "LGBMClassifier():  0.9067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "GNB = GaussianNB()\n",
    "rf = RandomForestClassifier()\n",
    "ada = AdaBoostClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "xgb = XGBClassifier()\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "for model in [tree, GNB, rf, ada, gb, xgb, lgbm]:\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_model = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred_model)\n",
    "    \n",
    "    print('{0}: {1: .4f}'.format(model, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9300ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_data = data[['BP', 'm_wt', 'px_h', 'px_w', 'ram', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78e3390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = sel_data.iloc[:,:-1]\n",
    "y_data = sel_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0888ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardscaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard = StandardScaler()\n",
    "X_scaled_standard = standard.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8f1a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_standard, y_data, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e5d4189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier():  0.8267\n",
      "GaussianNB():  0.7933\n",
      "RandomForestClassifier():  0.9233\n",
      "AdaBoostClassifier():  0.7167\n",
      "GradientBoostingClassifier():  0.9033\n",
      "[15:54:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None):  0.9200\n",
      "LGBMClassifier():  0.9167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "GNB = GaussianNB()\n",
    "rf = RandomForestClassifier()\n",
    "ada = AdaBoostClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "xgb = XGBClassifier()\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "for model in [tree, GNB, rf, ada, gb, xgb, lgbm]:\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_model = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred_model)\n",
    "    \n",
    "    print('{0}: {1: .4f}'.format(model, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d9d9cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:57:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None):  0.9200\n"
     ]
    }
   ],
   "source": [
    "model = xgb\n",
    "model.fit(X_train, y_train)\n",
    "pred_model = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred_model)\n",
    "\n",
    "print('{0}: {1: .4f}'.format(model, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9e097033",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Electronics_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "422645ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['dual','4G','3G','ts','wifi','blue'], inplace = True, axis=1)\n",
    "bye_outlier(df=df, column='front_c',weight=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "716d8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "ss = StandardScaler()\n",
    "scaled_X_standard = ss.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X_standard, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "66bf1980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:19:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None):  0.9329\n"
     ]
    }
   ],
   "source": [
    "model = xgb\n",
    "model.fit(X_train, y_train)\n",
    "pred_model = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred_model)\n",
    "\n",
    "print('{0}: {1: .4f}'.format(model, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fd7843c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:45:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9328859060402684\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
    "              gamma=0, gpu_id=-1, importance_type=None,\n",
    "              interaction_constraints='', learning_rate=0.300000012,\n",
    "              max_delta_step=0, max_depth=5, min_child_weight=1,\n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
    "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
    "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
    "              subsample=1, tree_method='exact', validate_parameters=1,\n",
    "              verbosity=None)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "preds = xgb.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f880b3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e1926964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:40:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_s = VotingClassifier(estimators=[('RF',rf), ('XGB',xgb),('LGBM',lgbm),('GB',gb),('LR',lr),('KNN',knn)], voting='soft')\n",
    "\n",
    "voting_s.fit(X_train, y_train)\n",
    "pred_s = voting_s.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2521306c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9295302013422819"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ab9afb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "lr = LogisticRegression()\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "726024f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9261744966442953"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_s = VotingClassifier(estimators=[('RF',rf), ('LR',lr),('KNN',knn)], voting='soft')\n",
    "\n",
    "voting_s.fit(X_train, y_train)\n",
    "pred_s = voting_s.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, pred_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "894098b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Electronics_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "10909ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_data = data[['BP', 'c_speed','m_int', 'm_wt', 'px_h', 'px_w', 'ram', 'talk_t', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "991df4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "ss = StandardScaler()\n",
    "scaled_X_standard = ss.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X_standard, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cb6a181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:55:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=5, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None):  0.9295\n"
     ]
    }
   ],
   "source": [
    "model = xgb\n",
    "model.fit(X_train, y_train)\n",
    "pred_model = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred_model)\n",
    "\n",
    "print('{0}: {1: .4f}'.format(model, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ab5e7b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier():  0.8523\n",
      "GaussianNB():  0.7852\n",
      "RandomForestClassifier():  0.9128\n",
      "AdaBoostClassifier():  0.7383\n",
      "GradientBoostingClassifier():  0.9060\n",
      "[16:56:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None):  0.9228\n",
      "LGBMClassifier():  0.9295\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "GNB = GaussianNB()\n",
    "rf = RandomForestClassifier()\n",
    "ada = AdaBoostClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "xgb = XGBClassifier()\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "for model in [tree, GNB, rf, ada, gb, xgb, lgbm]:\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_model = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred_model)\n",
    "    \n",
    "    print('{0}: {1: .4f}'.format(model, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "41eab766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:57:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9261744966442953"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_s = VotingClassifier(estimators=[('RF',rf), ('XGB',xgb),('LGBM',lgbm)], voting='soft')\n",
    "\n",
    "voting_s.fit(X_train, y_train)\n",
    "pred_s = voting_s.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, pred_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82954c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
